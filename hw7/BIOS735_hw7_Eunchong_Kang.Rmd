---
title: "HW 7 - Numerical Integration"
author: "Eunchong Kang"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, warning=FALSE, message=FALSE}
# Library set-up
library(statmod)
library(optimx)
library(numDeriv)
library(tidyverse)
```

# Maximization of poisson GLMM from lecture

Obtain the MLE's for $\boldsymbol{\beta}$ and $\sigma_{\gamma}^2$ from lecture.  


```{r}
# input data
setwd("D:/Spring_2022_D/BIOS735/hw/hw7")
alz = read.table("alzheimers.dat", header = T)
xi <- cbind(rep(1,5), 1:5)

# Starting value
beta = c(1.804, 0.165)
s2gamma = 0.000225
```

The function to be maximized is below.
$$L(\boldsymbol{\beta}, \sigma^2_{\gamma} | y) = \prod_{i = 1}^{22}\int \left[\left(\prod_{j = 1}^{5}f(y_{ij} | \lambda_{ij})\right)\phi(\gamma_i| 0, \sigma^2_{\gamma}) \right] d\gamma_i$$
where $f(y_{ij} | \lambda_{ij})$ is the pdf of poisson distribution. Put $\left(\prod_{j = 1}^{5}f(y_{ij} | \lambda_{ij})\right)\phi(\gamma_i| 0, \sigma^2_{\gamma})$ = $g(\gamma_i)$.
The likelihood is the product of 22 one-dimensional integrals with respect to $\gamma_i$.

And, the log-likelihood is:
$$\mathcal{l}(\boldsymbol{\beta}, \sigma^2_{\gamma} | y) = \sum_{i = 1}^{22}\log\left[\int \left[\left(\prod_{j = 1}^{5}f(y_{ij} | \lambda_{ij})\right)\phi(\gamma_i| 0, \sigma^2_{\gamma}) \right] d\gamma_i\right].$$
To find MLE for $\beta$ and $\sigma^2_\gamma$, adaptive Gaussian Quadrature can be used to approximate the integral by subject, and the approximated integrals will be maximized via Nelder-Mead method. The approximation of the integral for 'each subject' is below.

\begin{align}
\int g(t)dt &= \int h(t)\phi(t; \hat{\mu}, \hat{\sigma}^2)dt \\
&\approx \sum_{k = 1}^M A^*_kh(t_k)\\
&=\sum_{k = 1}^M A^*_kh\left(\hat{\mu} + \sqrt{2\hat{\sigma}^2}x_k\right)
\end{align}

Here, $t_k$ is node and $M$ is the number of nodes. $A_k$ is weight, where $A^*_k = A_k/\sqrt{\pi}$. $h(t) = \frac{g(t)}{\phi(t; \hat{\mu}, \hat{\sigma}^2)}$, and the new $w(x)$ is $\phi(t; \hat{\mu}, \hat{\sigma}^2)$. Also, $\hat{\mu}$ is the mode of the integrand $g(\gamma_i)$, and $\hat{\sigma^2} = \left(-\frac{\partial^2}{\partial t^2}\log(g(t))\rvert_{t=\hat{\mu}}\right)^{-1}$

## 1. Integration related helper functions

1-1. 'inner' function is $g(\gamma_i)$  
1-2. 'muhat' function is for $\hat{\mu}$  
1-3. 'sigma2hat' function is for $\hat{\sigma^2}$  
1-4. 'h_t' funtion is $h(t)$

```{r}
## 1. Integration related helper functions
# 1-1. Create integrand function for ith subject
# This needs to calculate muhat and sigma2hat for aGQ
inner <- function(gammai, yi, beta, s2gamma, log = F){
        val <- rep(NA, length(gammai))
        
        for (i in 1:length(val)){
                lambda <- exp(xi %*% beta + gammai[i])
                val0 <- dpois(x = yi, lambda = lambda)
                val[i] <- prod(val0) * dnorm(x = gammai[i],
                                             mean = 0,
                                             sd = sqrt(s2gamma))
        }
        
        if (log == F){
                return(val)
        } else{
                return(log(val))
        }
}

# 1-2. Create a function calculating the posterior mode of the integrand (=muhat)
muhat <- function(yi, beta, s2gamma) {
        pm <- suppressWarnings(
                optimx(
                        par = 0,
                        fn = function(x, yi, beta, s2gamma) {
                                inner(
                                        gammai = x,
                                        yi = yi,
                                        beta = beta,
                                        s2gamma = s2gamma
                                )
                        },
                        yi = yi,
                        beta = beta,
                        s2gamma = s2gamma,
                        method = "Nelder-Mead",
                        control = list(maximize = T)
                )
        )
        return(pm$p1)
}

# 1-3. Create a function calculating a sigma2hat
sigma2hat <- function(muhat, yi, beta, s2gamma, inner) {
        hess_pm <- hessian(
                func = inner,
                x = muhat,
                yi = yi,
                beta = beta,
                s2gamma = s2gamma,
                log = T
        )
        
        return(-1 / hess_pm)
}

# 1-4 Create h(x) function 
# Integrad factors into h(x)*pi(gammai|muhat,sigma2hat)
# t is adjusted nodes
h_t <- function(t, yi, beta, s2gamma, muhat, sigma2hat){
        val <- rep(NA, length(t))
        
        for (i in 1:length(val)){
                lambda <- exp(xi %*% beta + t[i])
                val0 <- dpois(x = yi, lambda = lambda)
                val[i] <- prod(val0) * 
                        dnorm(x = t[i], mean = 0, sd = sqrt(s2gamma)) / 
                        dnorm(x = t[i], mean = muhat, sd = sqrt(sigma2hat))
        }
        return(val)
}
```

## 2. Integration  
'integr_ll' function calculates an approximation of the 'log-likelihood' for all subjects given data, $\beta$, and $\sigma^2_{\gamma}$. The log-likelihood was used to find MLE because the likelihood is too small.

```{r}
## 2. Integration
# This function calculates Log-likelihood given beta and s2gamma
# theta consists of beta and s2gamma
# M is the number of nodes
# output is total Log-likelihood
integr_ll <- function(theta, M) {
        # Split theta into beta and s2gama
        beta <- theta[1:2]
        s2gamma <- theta[3]
        
        # Finds nodes and weights
        gh <- gauss.quad(n = M, kind = "hermite")
        nodes_over_gamma <- gh$nodes
        
        # Create NA vector
        val <- rep(NA, max(alz$subject))
        
        # Looping
        for (i in 1:max(alz$subject)) {
                # Subset data by subject
                yi <- alz$words[alz$subject == i]
                
                # Calculate muhat and sigma2hat by subject
                muhat_yi <- muhat(yi, beta, s2gamma)
                sigma2hat_yi <- sigma2hat(muhat_yi, yi, beta, s2gamma, inner)
                
                # Calculate new nodes and weights
                t <- muhat_yi + sqrt(2 * as.numeric(sigma2hat_yi)) * nodes_over_gamma
                w <- gh$weights / sqrt(pi)
                
                # Approximate the integral by subject
                val[i] <- sum(
                        w * h_t(
                                t = t,
                                yi = yi,
                                beta = beta,
                                s2gamma = s2gamma,
                                muhat = muhat_yi,
                                sigma2hat = sigma2hat_yi
                        )
                )
                
        }
        
        # Calculate Log-likelihood
        return(sum(log(val)))
        
        # Calculate Likelihood
        #return(prod(val))
}
```

## 3. Optimization using Nelder-Mead  
This step is for optimizing the whole log-likelihood to obtain MLE of $\beta$ and $\sigma^2_{\gamma}$. Nelder-Mead method was used because it doesn't need the derivatives. Also, M was equals to five because there was no significant difference when using M greater than 5.


```{r, warning=FALSE, results="hide"}
## 3. Optimization using Nelder-Mead
start <- Sys.time()
fit <- optimx(
        par = c(beta, s2gamma),
        fn = function(x, M){integr_ll(theta=x, M)},
        M = 5,
        method = "Nelder-Mead",
        control = list(maximize = T)
)
end <- Sys.time()
```

```{r}
# Print time needed to run the code
print(end - start)
```

## 4. Results  
Here, the results are shown below. It takes some time to get the result expecially when using the log-likelihood instead of the likelihood, so I include the several results as a table.

``` {r}
# results
fit
```

||M|$\beta_0$|$\beta_1$|$\hat{\sigma^2_{\gamma}}$|value|
|:--:|:--:|:--:|:--:|:--:|:--:|
|Log-likelihood|5|1.819893|0.1657523|0.218454|-281.9256|
|Log-likelihood|1|1.820355|0.165672|0.2177847|-281.9483|
|Likelihood|5|1.804|0.165|0.180625|3.03906e-123|

To sum up, there is no considerable difference between M = 5 and M =1 when using the log-likelihood. This is because adaptive Gaussian Quadrature was used. 


# Plot

Now, plot the fitted line from the fitted GLMM on the spaghetti plot from lecture

```{r}
# Data transformation
beta_fitted <- c(1.819893, 0.1657523)
alz_fitted <- data.frame(subject = factor(c(rep(23,5))), month = rep(1:5), words=exp(xi %*% beta_fitted))
alz$subject <- as.factor(alz$subject)

# make a plot
ggplot(alz, aes(month, words)) +
        geom_line(aes(color=subject, group=subject)) +
        geom_line(data = alz_fitted, aes(month, words), size=1) +
        theme(legend.position = "none") +
        labs(
                title = "Spaghetti plot with the fitted line",
                caption = "Black line is the fitted line"
        ) +
        coord_cartesian(ylim=c(0,25)) +
        scale_y_continuous(breaks=seq(0, 25, 5))

```